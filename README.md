# LLM Hallucination Detection System

This project detects and classifies hallucinations in LLM-generated text by verifying factual claims against real-world web evidence.  
It uses Natural Language Inference (NLI), Named Entity Recognition (NER) and semantic search to evaluate each sentence for factual consistency.



## Overview

Large Language Models (LLMs) can produce fluent text that sounds factual but isnâ€™t backed by real information.  
This system analyzes each sentence (or â€œclaimâ€) in a generated paragraph and classifies it as one of:

- ðŸŸ© Verified â€” Supported by online evidence  
- ðŸŸ¥ Hallucination â€” Contradicted by evidence or likely false  
- ðŸŸ¨ Uncertain â€” No sufficient supporting data found  

The verification process is powered by RoBERTa-Large-MNLI for entailment/contradiction detection and web snippets retrieved from DuckDuckGo.



## Core Pipeline

Each claim passes through the following stages:

1. Sentence Segmentation â€“ The paragraph is split into claims using spaCy.  
2. Search Query Generation â€“ Each claim is expanded into a refined search query using:
   - Named entities (from a BERT-based NER model)
   - Noun phrases (from spaCy)
   - Heuristics for keywords like â€œdiscoveredâ€, â€œinventedâ€, etc.
3. Web Search â€“ Queries are sent to DuckDuckGo using the `duckduckgo_search` library to retrieve contextual snippets.
4. NLI Evaluation â€“ Each snippet is compared to the claim using `roberta-large-mnli`:
   - Label **ENTAILMENT** â†’ supports the claim  
   - Label **CONTRADICTION** â†’ refutes the claim  
   - Label **NEUTRAL** â†’ irrelevant or insufficient evidence
5. Decision Logic 
   Based on maximum entailment and contradiction scores:
if contradiction >= 0.80 â†’ hallucination
elif entailment >= 0.75 â†’ verified
elif contradiction >= 0.75 â†’ hallucination
else â†’ uncertain
6. Visualization (Streamlit) â€“ The paragraph is color-coded:
- ðŸŸ© Green: Verified  
- ðŸŸ¨ Yellow: Uncertain  
- ðŸŸ¥ Red: Hallucination  



## Models Used
1. RoBERTa-large-MNLI (Natural Language Inference Model)
2. dslim/bert-base-NER (Named Entity Recognition Model)
3. all-MiniLM-L6-v2 (Sentence Transformer Model)
4. spaCyâ€™s en_core_web_sm Model



## Project Structure

LLM-Hallucination-Detection/
â”‚
â”œâ”€â”€ app.py # Streamlit web interface
â”œâ”€â”€ verifier.py # Core verification logic
â”œâ”€â”€ requirements.txt # All Python dependencies
â””â”€â”€ README.md # This documentation file
